{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1TnI4uAbrLx7",
    "outputId": "5ab1a556-1890-4100-a182-275721c8e454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras_nlp in /usr/local/lib/python3.8/dist-packages (0.3.0)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (2.10.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (21.3)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (1.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from keras_nlp) (1.23.2)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (1.1.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (1.6.3)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (3.7.0)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (0.4.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (14.0.6)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (0.26.0)\n",
      "Requirement already satisfied: keras<2.11,>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (2.10.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.11,>=2.10.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (2.10.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (1.1.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (0.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (2.0.7)\n",
      "Requirement already satisfied: tensorboard<2.11,>=2.10 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (2.10.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (1.48.1)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (4.3.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (3.19.4)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (1.14.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (65.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/lib/python3/dist-packages (from tensorflow->keras_nlp) (1.14.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow->keras_nlp) (3.3.0)\n",
      "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow-text->keras_nlp) (0.12.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->keras_nlp) (3.0.9)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/lib/python3/dist-packages (from astunparse>=1.6.0->tensorflow->keras_nlp) (0.34.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/lib/python3/dist-packages (from tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (2.22.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (2.2.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (0.4.6)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (2.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (0.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.8/dist-packages (from werkzeug>=1.0.1->tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (2.1.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (1.3.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (5.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (4.12.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (3.2.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard<2.11,>=2.10->tensorflow->keras_nlp) (3.8.1)\n",
      "\u001b[33mWARNING: You are using pip version 20.2.4; however, version 22.3 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kevRkb9aqXAd"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_nlp\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03WXT5hBrzM7",
    "outputId": "9efd22e1-dc05-474e-9bad-352fd839010d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Dans son cadre terni, le pâle Christ d'ivoire,\n",
      "    Cloué les bras en croix sur son étoffe noire,\n",
      "                Redoublait de pâleur;\n",
      "    Et comme au Golgotha, dans sa dure agonie,\n",
      "    Les muscles en relief de sa face jaunie\n",
      "                Se tordaient de douleur.Les Éditions de _Plaute_ et de _Christophe-Colomb_, comédies en 3 actes\n",
      "et en vers, et de _Baudouin_, _empereur_, tragédie en 3 actes, sont à\n",
      "refaire, ayant été détruites dans un incendie.    Partout est engagé le combat redoutable;\n",
      "    A l'heure harmonieuse où la terre s'endort,\n",
      "    Il rend la nuit sinistre et l'ombre épouvantable,\n",
      "    Tout brin d'herbe est un champ de carnage et de mort.Mon esprit, comme mes vertèbres,\n",
      "Invoque ardemment le repos;\n",
      "Le cœur plein de songes funèbres,    Toujours vous trouveriez, sous cette architecture,\n",
      "    Au milieu de la fange et de la pourriture,\n",
      "    Dans le suaire usé le cadavre tout droit,    Ces gens qui font des tours en plein air\n",
      "    Commencent à être rares à Paris\n",
      "    Dans ma jeun\n"
     ]
    }
   ],
   "source": [
    "texts = \"\"\n",
    "\n",
    "for(dirpath, dirnames, filenames) in os.walk(\"/poems\"):\n",
    "  for filename in filenames:\n",
    "    if filename.endswith('.txt'):\n",
    "        # print(filename)\n",
    "        path = os.path.join(dirpath, filename)\n",
    "        with open(path,'r') as file_stream:\n",
    "            texts += file_stream.read()\n",
    "    \n",
    "print(texts[0:1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "H9nsqeFzsEaX"
   },
   "outputs": [],
   "source": [
    "text_list = texts.split('.')\n",
    "text_list = list(filter(None, text_list))\n",
    "\n",
    "import random\n",
    "random.shuffle(text_list)\n",
    "length = len(text_list)\n",
    "#split the dataset \n",
    "text_train = text_list[:int(0.7*length)]\n",
    "text_test = text_list[int(0.7*length):int(0.85*length)]\n",
    "text_valid = text_list[int(0.85*length):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R6lp19ohs1Eg",
    "outputId": "a50401af-cfac-4f66-9305-07ef664e6f43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[   1 8730    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0]], shape=(1, 51), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "#def custom_standardization(input_string):\n",
    "#    sentence = tf.strings.lower(input_string)\n",
    "#    sentence = tf.strings.regex_replace(sentence, \"\\n\", \" \")\n",
    "#    return sentence\n",
    "\n",
    "maxlen = 50\n",
    "# You can also set calculate the longest sentence in the data - 25 in this case\n",
    "# maxlen = len(max(text_list).split(' ')) \n",
    "\n",
    "vectorize_layer = TextVectorization(\n",
    "#    standardize = custom_standardization,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "\n",
    "vectorize_layer.adapt(text_list)\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "vocab_size = len(vocab)\n",
    "index_lookup = dict(zip(range(len(vocab)), vocab))   \n",
    "#example vectorrizing words\n",
    "print(vectorize_layer(['hello world!']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Y2rIIKCHxJ05"
   },
   "outputs": [],
   "source": [
    "#DataSet creation\n",
    "def create_dataset(text, batch_size):\n",
    "  dataset = tf.data.Dataset.from_tensor_slices(text)\n",
    "  dataset = dataset.shuffle(buffer_size=256)\n",
    "  dataset = dataset.batch(batch_size)\n",
    "  return dataset \n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = create_dataset(text_train, batch_size)\n",
    "\n",
    "valid_dataset = create_dataset(text_valid, batch_size)\n",
    "\n",
    "test_dataset = create_dataset(text_test, batch_size)\n",
    "\n",
    "#preprocess text\n",
    "train_dataset = train_dataset.map(preprocess_text)\n",
    "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = test_dataset.map(preprocess_text)\n",
    "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "valid_dataset = valid_dataset.map(preprocess_text)\n",
    "valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yINsowoz_4V",
    "outputId": "59a48ebc-8aa5-43f5-f190-e6cceb6d9cfb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50)]              0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 50, 128)          5431296   \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " transformer_decoder (Transf  (None, 50, 128)          99584     \n",
      " ormerDecoder)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 50, 42382)         5467278   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,998,158\n",
      "Trainable params: 10,998,158\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "\n",
    "def create_model():\n",
    "    inputs = keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(vocab_size, maxlen, embed_dim)(inputs)\n",
    "    decoder = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim, \n",
    "                                                            num_heads=num_heads, \n",
    "                                                            dropout=0.5)(embedding_layer)\n",
    "    \n",
    "    outputs = keras.layers.Dense(vocab_size, activation='softmax')(decoder)\n",
    "    \n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=\"adam\", \n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "zLlH-CMp2OX-"
   },
   "outputs": [],
   "source": [
    "#Custom Callback\n",
    "class TextSampler(keras.callbacks.Callback):\n",
    "    def __init__(self, start_prompt, max_tokens):\n",
    "        self.start_prompt = start_prompt\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "    # Helper method to choose a word from the top K probable words with respect to their probabilities\n",
    "    # in a sequence\n",
    "    def sample_token(self, logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        decoded_sample = self.start_prompt\n",
    "        \n",
    "        for i in range(self.max_tokens-1):\n",
    "            tokenized_prompt = vectorize_layer([decoded_sample])[:, :-1]\n",
    "            predictions = self.model.predict([tokenized_prompt], verbose=0)\n",
    "            # To find the index of the next word in the prediction array.\n",
    "            # The tokenized prompt is already shorter than the original decoded sample\n",
    "            # by one, len(decoded_sample.split()) is two words ahead - so we remove 1 to get\n",
    "            # the next word in the sequence\n",
    "            sample_index = len(decoded_sample.strip().split())-1\n",
    "            \n",
    "            sampled_token = self.sample_token(predictions[0][sample_index])\n",
    "            sampled_token = index_lookup[sampled_token]\n",
    "            decoded_sample += \" \" + sampled_token\n",
    "            \n",
    "        print(f\"\\nSample text:\\n{decoded_sample}...\\n\")\n",
    "\n",
    "# First 5 words of a random sentence to be used as a seed\n",
    "random_sentence = ' '.join(random.choice(text_valid).replace('\\n', ' ').split(' ')[:4])\n",
    "sampler = TextSampler(random_sentence, 30)\n",
    "reducelr = keras.callbacks.ReduceLROnPlateau(patience=10, monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CU_IdIed2kqD",
    "outputId": "b834f2e6-0471-40ac-b201-4854b121fcd4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "155/155 [==============================] - ETA: 0s - loss: 5.2729 - perplexity: 194.9877 - accuracy: 0.4785\n",
      "Sample text:\n",
      "  Beauté à de et de et et le les les le et les le le le le et et les de le le et le la de la et et et...\n",
      "\n",
      "155/155 [==============================] - 239s 2s/step - loss: 5.2729 - perplexity: 194.9877 - accuracy: 0.4785 - val_loss: 3.8981 - val_perplexity: 49.3091 - val_accuracy: 0.5007 - lr: 0.0010\n",
      "Epoch 2/3\n",
      "155/155 [==============================] - ETA: 0s - loss: 3.6970 - perplexity: 40.3267 - accuracy: 0.5078\n",
      "Sample text:\n",
      "  Beauté à la mer à sa main des pieds  les yeux en son front dans un ciel  de la mer  de sa voix  qui ne te fait...\n",
      "\n",
      "155/155 [==============================] - 243s 2s/step - loss: 3.6970 - perplexity: 40.3267 - accuracy: 0.5078 - val_loss: 3.6764 - val_perplexity: 39.5046 - val_accuracy: 0.5183 - lr: 0.0010\n",
      "Epoch 3/3\n",
      "155/155 [==============================] - ETA: 0s - loss: 3.3251 - perplexity: 27.8030 - accuracy: 0.5265\n",
      "Sample text:\n",
      "  Beauté à ce sont plus que les grands plus de son cœur le temps du soir le soir et des hommes qui se sont des yeux  de leurs bras de...\n",
      "\n",
      "155/155 [==============================] - 236s 2s/step - loss: 3.3251 - perplexity: 27.8030 - accuracy: 0.5265 - val_loss: 3.6086 - val_perplexity: 36.9138 - val_accuracy: 0.5224 - lr: 0.0010\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "history = model.fit(train_dataset, \n",
    "                    validation_data=valid_dataset,\n",
    "                    epochs=3, \n",
    "                    callbacks=[sampler, reducelr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "RxClb-s37lfV"
   },
   "outputs": [],
   "source": [
    "def sample_token(logits):\n",
    "        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "def generate_text(prompt, response_length=20):\n",
    "    decoded_sample = prompt\n",
    "    for i in range(response_length-1):\n",
    "        tokenized_prompt = vectorize_layer([decoded_sample])[:, :-1]\n",
    "        predictions = model.predict([tokenized_prompt], verbose=0)\n",
    "        sample_index = len(decoded_sample.strip().split())-1\n",
    "\n",
    "        sampled_token = sample_token(predictions[0][sample_index])\n",
    "        sampled_token = index_lookup[sampled_token]\n",
    "        decoded_sample += \" \" + sampled_token\n",
    "    return decoded_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtPCaw3z7tkn",
    "outputId": "7c553cd3-fc8f-47ed-c284-be7f76aa3f1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les fleurs sans le ciel et la terre en vain le soleil dans les jours du\n",
      "Combien d'entre vous vous en un soir le temps que nous avons la vie à la vie\n",
      "Le corbeau sur un arbre perché tenait en son bec un jour à son\n",
      "Ce matin là, la première chose de sa tête dans le vent et la vie\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"J'aime les fleurs\", 15))\n",
    "print(generate_text(\"Combien d'entre vous\", 15))\n",
    "print(generate_text(\"Le corbeau sur un arbre perché tenait en son bec un\", 4))\n",
    "print(generate_text(\"Ce matin là, la première chose\", 10))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
