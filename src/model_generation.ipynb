{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install keras_nlp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TnI4uAbrLx7",
        "outputId": "5ab1a556-1890-4100-a182-275721c8e454"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting keras_nlp\n",
            "  Downloading keras_nlp-0.3.0-py3-none-any.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 10.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras_nlp) (1.21.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from keras_nlp) (21.3)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from keras_nlp) (1.3.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from keras_nlp) (2.9.2)\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9 MB 52.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->keras_nlp) (3.0.9)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.14.1)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (2.9.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.1.2)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (2.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (57.4.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (14.0.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.15.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (0.27.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (2.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (3.3.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (3.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (0.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (4.1.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->keras_nlp) (1.49.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow->keras_nlp) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow->keras_nlp) (1.5.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (3.9.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (2022.9.24)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow->keras_nlp) (3.2.1)\n",
            "Requirement already satisfied: tensorflow-hub>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-text->keras_nlp) (0.12.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 578.0 MB 18 kB/s \n",
            "\u001b[?25hINFO: pip is looking at multiple versions of tensorflow-text to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-text\n",
            "  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.6 MB 55.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: tensorflow-text, keras-nlp\n",
            "Successfully installed keras-nlp-0.3.0 tensorflow-text-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "kevRkb9aqXAd"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras_nlp\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "crime_and_punishment_url = 'https://www.gutenberg.org/files/2554/2554-0.txt'\n",
        "brothers_of_karamazov_url = 'https://www.gutenberg.org/files/28054/28054-0.txt'\n",
        "the_idiot_url = 'https://www.gutenberg.org/files/2638/2638-0.txt'\n",
        "the_possessed_url = 'https://www.gutenberg.org/files/8117/8117-0.txt'\n",
        "\n",
        "paths = [crime_and_punishment_url, brothers_of_karamazov_url, the_idiot_url, the_possessed_url]\n",
        "names = ['Crime and Punishment', 'Brothers of Karamazov', 'The Idiot', 'The Possessed']\n",
        "texts = ''\n",
        "for index, path in enumerate(paths):\n",
        "    filepath = keras.utils.get_file(f'{names[index]}.txt', origin=path)\n",
        "    text = ''\n",
        "    with open(filepath, encoding='utf-8') as f:\n",
        "        text = f.read()\n",
        "        # First 50 lines are the Gutenberg intro and preface\n",
        "        # Skipping first 10k characters for each book should be approximately\n",
        "        # removing the intros and prefaces.\n",
        "        texts += text[10000:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "03WXT5hBrzM7",
        "outputId": "9efd22e1-dc05-474e-9bad-352fd839010d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.gutenberg.org/files/2554/2554-0.txt\n",
            "1201520/1201520 [==============================] - 0s 0us/step\n",
            "Downloading data from https://www.gutenberg.org/files/28054/28054-0.txt\n",
            "2041953/2041953 [==============================] - 0s 0us/step\n",
            "Downloading data from https://www.gutenberg.org/files/2638/2638-0.txt\n",
            "1427675/1427675 [==============================] - 0s 0us/step\n",
            "Downloading data from https://www.gutenberg.org/files/8117/8117-0.txt\n",
            "1483181/1483181 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = texts.split('.')\n",
        "text_list = list(filter(None, text_list))\n",
        "\n",
        "import random\n",
        "random.shuffle(text_list)\n",
        "length = len(text_list)\n",
        "#split the dataset \n",
        "text_train = text_list[:int(0.7*length)]\n",
        "text_test = text_list[int(0.7*length):int(0.85*length)]\n",
        "text_valid = text_list[int(0.85*length):]"
      ],
      "metadata": {
        "id": "H9nsqeFzsEaX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "#def custom_standardization(input_string):\n",
        "#    sentence = tf.strings.lower(input_string)\n",
        "#    sentence = tf.strings.regex_replace(sentence, \"\\n\", \" \")\n",
        "#    return sentence\n",
        "\n",
        "maxlen = 50\n",
        "# You can also set calculate the longest sentence in the data - 25 in this case\n",
        "# maxlen = len(max(text_list).split(' ')) \n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "#    standardize = custom_standardization,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=maxlen + 1,\n",
        ")\n",
        "\n",
        "vectorize_layer.adapt(text_list)\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "vocab_size = len(vocab)\n",
        "index_lookup = dict(zip(range(len(vocab)), vocab))   \n",
        "#example vectorrizing words\n",
        "print(vectorize_layer(['hello world!']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R6lp19ohs1Eg",
        "outputId": "a50401af-cfac-4f66-9305-07ef664e6f43"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[  1 319   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]], shape=(1, 51), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DataSet creation\n",
        "def create_dataset(text, batch_size):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(text)\n",
        "  dataset = dataset.shuffle(buffer_size=256)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset \n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "train_dataset = create_dataset(text_train, batch_size)\n",
        "\n",
        "valid_dataset = create_dataset(text_valid, batch_size)\n",
        "\n",
        "test_dataset = create_dataset(text_test, batch_size)\n",
        "\n",
        "#preprocess text\n",
        "train_dataset = train_dataset.map(preprocess_text)\n",
        "train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_dataset = test_dataset.map(preprocess_text)\n",
        "test_dataset = test_dataset.prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "valid_dataset = valid_dataset.map(preprocess_text)\n",
        "valid_dataset = valid_dataset.prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "Y2rIIKCHxJ05"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "num_heads = 4\n",
        "\n",
        "def create_model():\n",
        "    inputs = keras.layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
        "    embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(vocab_size, maxlen, embed_dim)(inputs)\n",
        "    decoder = keras_nlp.layers.TransformerDecoder(intermediate_dim=embed_dim, \n",
        "                                                            num_heads=num_heads, \n",
        "                                                            dropout=0.5)(embedding_layer)\n",
        "    \n",
        "    outputs = keras.layers.Dense(vocab_size, activation='softmax')(decoder)\n",
        "    \n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer=\"adam\", \n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=[keras_nlp.metrics.Perplexity(), 'accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yINsowoz_4V",
        "outputId": "59a48ebc-8aa5-43f5-f190-e6cceb6d9cfb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 50)]              0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, 50, 128)          3861888   \n",
            " g (TokenAndPositionEmbeddin                                     \n",
            " g)                                                              \n",
            "                                                                 \n",
            " transformer_decoder (Transf  (None, 50, 128)          99584     \n",
            " ormerDecoder)                                                   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 50, 30121)         3885609   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,847,081\n",
            "Trainable params: 7,847,081\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Custom Callback\n",
        "class TextSampler(keras.callbacks.Callback):\n",
        "    def __init__(self, start_prompt, max_tokens):\n",
        "        self.start_prompt = start_prompt\n",
        "        self.max_tokens = max_tokens\n",
        "        \n",
        "    # Helper method to choose a word from the top K probable words with respect to their probabilities\n",
        "    # in a sequence\n",
        "    def sample_token(self, logits):\n",
        "        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        return np.random.choice(indices, p=preds)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        decoded_sample = self.start_prompt\n",
        "        \n",
        "        for i in range(self.max_tokens-1):\n",
        "            tokenized_prompt = vectorize_layer([decoded_sample])[:, :-1]\n",
        "            predictions = self.model.predict([tokenized_prompt], verbose=0)\n",
        "            # To find the index of the next word in the prediction array.\n",
        "            # The tokenized prompt is already shorter than the original decoded sample\n",
        "            # by one, len(decoded_sample.split()) is two words ahead - so we remove 1 to get\n",
        "            # the next word in the sequence\n",
        "            sample_index = len(decoded_sample.strip().split())-1\n",
        "            \n",
        "            sampled_token = self.sample_token(predictions[0][sample_index])\n",
        "            sampled_token = index_lookup[sampled_token]\n",
        "            decoded_sample += \" \" + sampled_token\n",
        "            \n",
        "        print(f\"\\nSample text:\\n{decoded_sample}...\\n\")\n",
        "\n",
        "# First 5 words of a random sentence to be used as a seed\n",
        "random_sentence = ' '.join(random.choice(text_valid).replace('\\n', ' ').split(' ')[:4])\n",
        "sampler = TextSampler(random_sentence, 30)\n",
        "reducelr = keras.callbacks.ReduceLROnPlateau(patience=10, monitor='val_loss')"
      ],
      "metadata": {
        "id": "zLlH-CMp2OX-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "history = model.fit(train_dataset, \n",
        "                    validation_data=valid_dataset,\n",
        "                    epochs=3, \n",
        "                    callbacks=[sampler, reducelr])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CU_IdIed2kqD",
        "outputId": "b834f2e6-0471-40ac-b201-4854b121fcd4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n",
            "633/634 [============================>.] - ETA: 0s - loss: 2.4861 - perplexity: 12.0139 - accuracy: 0.6826\n",
            "Sample text:\n",
            " Oh, well, we know what is that you  are all that he would not be a little and he would come  for you are a very good thing and not...\n",
            "\n",
            "634/634 [==============================] - 83s 122ms/step - loss: 2.4861 - perplexity: 12.0142 - accuracy: 0.6826 - val_loss: 1.9596 - val_perplexity: 7.0964 - val_accuracy: 0.6972 - lr: 0.0010\n",
            "Epoch 2/3\n",
            "633/634 [============================>.] - ETA: 0s - loss: 1.8398 - perplexity: 6.2953 - accuracy: 0.7034\n",
            "Sample text:\n",
            " Oh, well, we are you  to the other people who have come to make up my own way i shall not at last night    to the first ...\n",
            "\n",
            "634/634 [==============================] - 79s 125ms/step - loss: 1.8398 - perplexity: 6.2954 - accuracy: 0.7034 - val_loss: 1.9034 - val_perplexity: 6.7085 - val_accuracy: 0.7001 - lr: 0.0010\n",
            "Epoch 3/3\n",
            "633/634 [============================>.] - ETA: 0s - loss: 1.7248 - perplexity: 5.6114 - accuracy: 0.7085\n",
            "Sample text:\n",
            " Oh, well, we will come in my own way  i don’t care of the same day  of you are in the same  time to be the same time and...\n",
            "\n",
            "634/634 [==============================] - 79s 125ms/step - loss: 1.7248 - perplexity: 5.6112 - accuracy: 0.7085 - val_loss: 1.8946 - val_perplexity: 6.6501 - val_accuracy: 0.7010 - lr: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sample_token(logits):\n",
        "        logits, indices = tf.math.top_k(logits, k=5, sorted=True)\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        return np.random.choice(indices, p=preds)\n",
        "\n",
        "def generate_text(prompt, response_length=20):\n",
        "    decoded_sample = prompt\n",
        "    for i in range(response_length-1):\n",
        "        tokenized_prompt = vectorize_layer([decoded_sample])[:, :-1]\n",
        "        predictions = model.predict([tokenized_prompt], verbose=0)\n",
        "        sample_index = len(decoded_sample.strip().split())-1\n",
        "\n",
        "        sampled_token = sample_token(predictions[0][sample_index])\n",
        "        sampled_token = index_lookup[sampled_token]\n",
        "        decoded_sample += \" \" + sampled_token\n",
        "    return decoded_sample"
      ],
      "metadata": {
        "id": "RxClb-s37lfV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text('the old woman '))\n",
        "print(generate_text('The truth is '))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtPCaw3z7tkn",
        "outputId": "7c553cd3-fc8f-47ed-c284-be7f76aa3f1c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the old woman  of the prince was very fond to her in the prince   and that i should not know\n",
            "The truth is  the other  people who can be no doubt of it and i know  what a great excitement\n"
          ]
        }
      ]
    }
  ]
}